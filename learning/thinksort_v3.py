"""
ThinkSort v3: Global Workspace Universal Transformer

Fixes "Hidden State Decay" in recurrent transformers with:
1. Residual Input Injection - original input re-injected at every thinking step
2. Step-Wise Sorting Reward - rewards for making progress toward sorted state
3. Adaptive Exit - stop thinking when confidence is high enough
4. KV-Cache Persistence - attention to original tokens preserved

Key Insight:
- Traditional recurrent: state degrades over repeated application
- ThinkSort v3: original problem ALWAYS visible via input injection

Goal: 100% accuracy on variable-length sorting with ~50k parameters
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, field
from collections import deque
from enum import IntEnum
import time


class ThinkAction(IntEnum):
    CONTINUE = 0  # Keep thinking
    OUTPUT = 1    # Output answer now


@dataclass
class ThinkSortConfig:
    """Configuration for ThinkSort v3."""
    # Model - TINY but powerful
    d_model: int = 64
    n_heads: int = 4
    d_ff: int = 256
    dropout: float = 0.0
    
    # Task
    vocab_size: int = 10
    max_seq_len: int = 64
    
    # Thinking
    min_think_steps: int = 2
    max_think_steps: int = 20  # Can think up to 20 steps
    confidence_threshold: float = 0.9  # Exit when confident
    
    # Rewards
    step_wise_reward: bool = True  # Reward sorting progress
    think_penalty: float = 0.005   # Small cost per think step
    
    # Training
    learning_rate: float = 1e-3
    weight_decay: float = 1e-4
    num_parallel_envs: int = 256
    
    # MuZero
    unroll_steps: int = 4
    discount: float = 1.0
    
    device: str = "cuda"


class GlobalWorkspaceBlock(nn.Module):
    """
    Transformer block with Global Workspace (residual input injection).
    
    At every thinking step:
    1. Original input is re-injected (prevents forgetting)
    2. Think step embedding added
    3. Standard attention + FFN
    
    This ensures the model NEVER forgets what problem it's solving.
    """
    
    def __init__(self, d_model: int, n_heads: int, d_ff: int, 
                 max_steps: int = 24, max_len: int = 64, dropout: float = 0.0):
        super().__init__()
        
        # Step embedding
        self.step_embed = nn.Embedding(max_steps, d_model)
        
        # Multi-head attention
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        self.scale = self.head_dim ** -0.5
        
        self.qkv = nn.Linear(d_model, 3 * d_model)
        self.out_proj = nn.Linear(d_model, d_model)
        
        # Relative position bias
        self.max_len = max_len
        self.rel_pos_bias = nn.Parameter(torch.zeros(2 * max_len - 1))
        nn.init.normal_(self.rel_pos_bias, std=0.02)
        
        # Input injection gate - learns how much to mix in original input
        self.input_gate = nn.Sequential(
            nn.Linear(d_model * 2, d_model),
            nn.Sigmoid()
        )
        
        # FFN
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout)
        )
        
        self.ln1 = nn.LayerNorm(d_model)
        self.ln2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
    
    def _get_rel_pos_bias(self, seq_len: int, device: torch.device) -> torch.Tensor:
        positions = torch.arange(seq_len, device=device)
        rel_dist = positions.unsqueeze(1) - positions.unsqueeze(0)
        rel_dist = rel_dist + (self.max_len - 1)
        rel_dist = rel_dist.clamp(0, 2 * self.max_len - 2)
        return self.rel_pos_bias[rel_dist]
    
    def forward(self, x: torch.Tensor, original_input: torch.Tensor, 
                think_step: int) -> torch.Tensor:
        """
        Forward with residual input injection.
        
        Args:
            x: current hidden state (batch, seq_len, d_model)
            original_input: original embedded input (batch, seq_len, d_model)
            think_step: which thinking iteration
        """
        batch_size, seq_len, _ = x.shape
        
        # 1. RESIDUAL INPUT INJECTION - the key innovation!
        # Gate controls how much original input to mix in
        gate_input = torch.cat([x, original_input], dim=-1)
        gate = self.input_gate(gate_input)  # (batch, seq_len, d_model)
        
        # Mix current state with original input
        x = x * (1 - gate) + original_input * gate
        
        # 2. Add step embedding
        clamped_step = min(think_step, self.step_embed.num_embeddings - 1)
        step_emb = self.step_embed(
            torch.full((batch_size,), clamped_step, device=x.device, dtype=torch.long)
        ).unsqueeze(1)
        x = x + step_emb
        
        # 3. Self-attention
        qkv = self.qkv(x).reshape(batch_size, seq_len, 3, self.n_heads, self.head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        
        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale
        rel_bias = self._get_rel_pos_bias(seq_len, x.device)
        attn_scores = attn_scores + rel_bias.unsqueeze(0).unsqueeze(0)
        
        attn_probs = F.softmax(attn_scores, dim=-1)
        attn_probs = self.dropout(attn_probs)
        
        attn_out = torch.matmul(attn_probs, v)
        attn_out = attn_out.transpose(1, 2).reshape(batch_size, seq_len, -1)
        attn_out = self.out_proj(attn_out)
        
        # 4. Residual + FFN
        x = self.ln1(x + self.dropout(attn_out))
        x = self.ln2(x + self.ffn(x))
        
        return x


class ThinkSortTransformer(nn.Module):
    """
    ThinkSort v3: World's Most Efficient Sorting Transformer
    
    Features:
    - Single shared block with global workspace
    - Residual input injection (never forgets the problem)
    - Adaptive computation (stops when confident)
    - ~50k parameters only!
    """
    
    def __init__(self, config: ThinkSortConfig):
        super().__init__()
        self.config = config
        
        # Input embedding
        self.token_embed = nn.Embedding(config.vocab_size + 1, config.d_model)
        
        # Output position embedding
        self.out_pos_embed = nn.Embedding(config.max_seq_len, config.d_model)
        
        # THE CORE: Single shared thinking block with global workspace
        self.think_block = GlobalWorkspaceBlock(
            config.d_model, config.n_heads, config.d_ff,
            config.max_think_steps, config.max_seq_len, config.dropout
        )
        
        # Output heads
        self.ln_out = nn.LayerNorm(config.d_model)
        self.policy_head = nn.Linear(config.d_model, config.vocab_size)
        self.value_head = nn.Linear(config.d_model, 1)
        
        # Confidence predictor (for adaptive exit)
        self.confidence_head = nn.Sequential(
            nn.Linear(config.d_model, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
        
        self._init_weights()
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight, gain=0.5)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Embedding):
                nn.init.normal_(m.weight, mean=0.0, std=0.02)
    
    def count_parameters(self) -> int:
        return sum(p.numel() for p in self.parameters())
    
    def forward(self, x: torch.Tensor, out_pos: int = 0, 
                adaptive_exit: bool = True) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int, float]:
        """
        Forward with adaptive thinking.
        
        Returns: (policy, value, hidden, think_steps, avg_confidence)
        """
        batch_size, seq_len = x.shape
        
        # Embed input - this will be preserved in global workspace
        original_input = self.token_embed(x)
        
        # Add output position embedding
        out_pos_tensor = torch.full((batch_size,), min(out_pos, self.config.max_seq_len - 1),
                                    device=x.device, dtype=torch.long)
        h = original_input + self.out_pos_embed(out_pos_tensor).unsqueeze(1)
        
        # Store original for injection
        original_for_injection = original_input.detach().clone()
        
        # Adaptive thinking loop
        think_steps = 0
        confidences = []
        
        for step in range(self.config.max_think_steps):
            # Apply thinking block with input injection
            h = self.think_block(h, original_for_injection, step)
            think_steps += 1
            
            # Check confidence for adaptive exit
            pooled = h.mean(dim=1)
            confidence = self.confidence_head(pooled).squeeze(-1)  # (batch,)
            avg_conf = confidence.mean().item()
            confidences.append(avg_conf)
            
            # Early exit if confident enough (and past minimum steps)
            if adaptive_exit and step >= self.config.min_think_steps - 1:
                if avg_conf >= self.config.confidence_threshold:
                    break
        
        h = self.ln_out(h)
        pooled = h.mean(dim=1)
        
        policy = self.policy_head(pooled)
        value = self.value_head(pooled)
        
        avg_confidence = np.mean(confidences) if confidences else 0.0
        
        return policy, value, h, think_steps, avg_confidence
    
    def forward_fixed_steps(self, x: torch.Tensor, out_pos: int = 0, 
                           num_steps: int = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Forward with fixed number of thinking steps."""
        if num_steps is None:
            num_steps = self.config.min_think_steps
        
        batch_size, seq_len = x.shape
        
        original_input = self.token_embed(x)
        out_pos_tensor = torch.full((batch_size,), min(out_pos, self.config.max_seq_len - 1),
                                    device=x.device, dtype=torch.long)
        h = original_input + self.out_pos_embed(out_pos_tensor).unsqueeze(1)
        
        original_for_injection = original_input.detach().clone()
        
        for step in range(num_steps):
            h = self.think_block(h, original_for_injection, step)
        
        h = self.ln_out(h)
        pooled = h.mean(dim=1)
        
        policy = self.policy_head(pooled)
        value = self.value_head(pooled)
        
        return policy, value, h


class VectorizedSortingEnv:
    """Sorting environment with step-wise sorting progress tracking."""
    
    def __init__(self, num_envs: int, seq_length: int, vocab_size: int = 10, device: str = "cuda"):
        self.num_envs = num_envs
        self.seq_length = seq_length
        self.vocab_size = vocab_size
        self.device = device
        
        self.input_seqs = None
        self.target_seqs = None
        self.positions = None
        self.outputs = None  # Track what's been output so far
    
    def reset(self) -> torch.Tensor:
        self.input_seqs = torch.randint(0, self.vocab_size, (self.num_envs, self.seq_length),
                                        device=self.device, dtype=torch.long)
        self.target_seqs, _ = torch.sort(self.input_seqs, dim=1)
        self.positions = torch.zeros(self.num_envs, device=self.device, dtype=torch.long)
        self.outputs = torch.zeros((self.num_envs, self.seq_length), device=self.device, dtype=torch.long)
        return self.input_seqs
    
    def step(self, actions: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Returns: (rewards, dones, correct, sorting_progress)
        
        sorting_progress: how well-sorted the output is so far (0-1)
        """
        batch_idx = torch.arange(self.num_envs, device=self.device)
        correct_actions = self.target_seqs[batch_idx, self.positions]
        
        correct = (actions == correct_actions)
        rewards = torch.where(correct, torch.ones_like(actions, dtype=torch.float32),
                             -torch.ones_like(actions, dtype=torch.float32))
        
        # Store output
        self.outputs[batch_idx, self.positions] = actions
        
        self.positions = self.positions + 1
        dones = (self.positions >= self.seq_length)
        
        # Calculate sorting progress: fraction of correctly placed elements
        current_pos = self.positions.clone()
        matches = (self.outputs[:, :self.seq_length] == self.target_seqs[:, :self.seq_length]).float()
        # Only count up to current position
        mask = torch.arange(self.seq_length, device=self.device).unsqueeze(0) < current_pos.unsqueeze(1)
        sorting_progress = (matches * mask.float()).sum(dim=1) / current_pos.float().clamp(min=1)
        
        return rewards, dones, correct.float(), sorting_progress


@dataclass
class ThinkingMetrics:
    """Track thinking steps and confidence."""
    samples: List[int] = field(default_factory=list)
    seq_lengths: List[int] = field(default_factory=list)
    avg_think_steps: List[float] = field(default_factory=list)
    avg_confidence: List[float] = field(default_factory=list)
    accuracies: List[float] = field(default_factory=list)
    
    def add(self, samples: int, seq_len: int, think: float, conf: float, acc: float):
        self.samples.append(samples)
        self.seq_lengths.append(seq_len)
        self.avg_think_steps.append(think)
        self.avg_confidence.append(conf)
        self.accuracies.append(acc)
    
    def print_summary(self):
        print("\n" + "="*70)
        print("ThinkSort v3 - Thinking Analysis")
        print("="*70)
        print(f"{'Samples':>10} | {'SeqLen':>6} | {'ThinkSteps':>10} | {'Confidence':>10} | {'Accuracy':>8}")
        print("-"*70)
        for i in range(0, len(self.samples), max(1, len(self.samples) // 10)):
            print(f"{self.samples[i]:>10,} | {self.seq_lengths[i]:>6} | "
                  f"{self.avg_think_steps[i]:>10.2f} | {self.avg_confidence[i]:>10.3f} | "
                  f"{self.accuracies[i]:>7.2%}")
        print("="*70)


class ThinkSortTrainer:
    """
    Trainer for ThinkSort v3.
    
    Features:
    - Step-wise sorting rewards (reward progress toward sorted output)
    - Adaptive exit based on confidence
    - Curriculum learning from 4 to 32 numbers
    """
    
    def __init__(self, config: ThinkSortConfig = None):
        if config is None:
            config = ThinkSortConfig()
        
        self.config = config
        self.device = config.device
        
        print(f"üß† ThinkSort v3: Global Workspace")
        print(f"   Residual Input Injection: ENABLED")
        print(f"   Adaptive Exit: ENABLED (threshold={config.confidence_threshold})")
        print(f"   Step-Wise Reward: {config.step_wise_reward}")
        print(f"   Think steps: {config.min_think_steps} to {config.max_think_steps}")
        
        self.model = ThinkSortTransformer(config).to(config.device)
        print(f"   Parameters: {self.model.count_parameters():,}")
        
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
        
        # Curriculum
        self.min_seq_length = 4
        self.max_seq_length = 32
        self.current_seq_length = self.min_seq_length
        self.accuracy_threshold = 0.90
        
        self.envs = VectorizedSortingEnv(
            config.num_parallel_envs, self.current_seq_length,
            config.vocab_size, config.device
        )
        
        # Replay buffer
        self.buffer = {'obs': [], 'actions': [], 'rewards': [], 'policies': []}
        self.buffer_size = 30000
        
        # Metrics
        self.total_samples = 0
        self.recent_accuracies = deque(maxlen=100)
        self.recent_think_steps = deque(maxlen=100)
        self.recent_confidence = deque(maxlen=100)
        self.think_metrics = ThinkingMetrics()
    
    def _recreate_env(self, new_len: int):
        self.current_seq_length = new_len
        self.envs = VectorizedSortingEnv(
            self.config.num_parallel_envs, new_len,
            self.config.vocab_size, self.device
        )
        self.buffer = {'obs': [], 'actions': [], 'rewards': [], 'policies': []}
    
    @torch.no_grad()
    def collect_batch(self) -> Tuple[float, float, float]:
        """Collect trajectories, return (accuracy, avg_think_steps, avg_confidence)."""
        self.model.eval()
        
        obs = self.envs.reset()
        targets = self.envs.target_seqs
        seq_len = self.current_seq_length
        
        all_actions, all_rewards, all_policies = [], [], []
        total_correct = 0
        total_think_steps = 0
        total_confidence = 0
        
        for step in range(seq_len):
            target_action = targets[:, step]
            
            # Expert policy
            expert_policy = torch.zeros(self.config.num_parallel_envs, self.config.vocab_size, device=self.device)
            expert_policy.scatter_(1, target_action.unsqueeze(1), 1.0)
            
            # Forward with adaptive thinking
            policy_logits, value, hidden, think_steps, confidence = self.model(
                obs, out_pos=step, adaptive_exit=True
            )
            
            total_think_steps += think_steps
            total_confidence += confidence
            
            # Sample action
            rand = np.random.random()
            if rand < 0.4:
                actions = target_action
            elif rand < 0.5:
                actions = torch.randint(0, self.config.vocab_size, 
                                       (self.config.num_parallel_envs,), device=self.device)
            else:
                actions = policy_logits.argmax(dim=-1)
            
            all_actions.append(actions)
            all_policies.append(expert_policy)
            
            rewards, dones, correct, sorting_progress = self.envs.step(actions)
            
            # Step-wise reward: bonus for sorting progress
            if self.config.step_wise_reward:
                progress_bonus = sorting_progress * 0.5  # Small bonus for correct sorting so far
                rewards = rewards + progress_bonus
            
            all_rewards.append(rewards)
            total_correct += correct.sum().item()
        
        # Store
        self.buffer['obs'].append(obs.cpu())
        self.buffer['actions'].append(torch.stack(all_actions, dim=1).cpu())
        self.buffer['rewards'].append(torch.stack(all_rewards, dim=1).cpu())
        self.buffer['policies'].append(torch.stack(all_policies, dim=1).cpu())
        
        max_entries = self.buffer_size // self.config.num_parallel_envs
        for k in self.buffer:
            if len(self.buffer[k]) > max_entries:
                self.buffer[k] = self.buffer[k][-max_entries:]
        
        self.total_samples += self.config.num_parallel_envs * seq_len
        accuracy = total_correct / (self.config.num_parallel_envs * seq_len)
        avg_think = total_think_steps / seq_len
        avg_conf = total_confidence / seq_len
        
        self.recent_accuracies.append(accuracy)
        self.recent_think_steps.append(avg_think)
        self.recent_confidence.append(avg_conf)
        
        return accuracy, avg_think, avg_conf
    
    def train_step(self) -> float:
        if len(self.buffer['obs']) < 2:
            return 0.0
        
        self.model.train()
        
        idx = np.random.randint(0, len(self.buffer['obs']))
        obs = self.buffer['obs'][idx].to(self.device)
        actions = self.buffer['actions'][idx].to(self.device)
        rewards = self.buffer['rewards'][idx].to(self.device)
        policies = self.buffer['policies'][idx].to(self.device)
        
        seq_len = actions.size(1)
        batch_size = obs.size(0)
        
        # Adaptive thinking based on seq length
        think_steps = max(2, seq_len // 2)
        
        # Value targets
        value_targets = torch.zeros(batch_size, seq_len, device=self.device)
        running = torch.zeros(batch_size, device=self.device)
        for t in reversed(range(seq_len)):
            running = rewards[:, t] + self.config.discount * running
            value_targets[:, t] = running
        
        # Forward
        policy_logits, value_pred, hidden = self.model.forward_fixed_steps(obs, out_pos=0, num_steps=think_steps)
        
        target_idx = policies[:, 0].argmax(dim=-1)
        policy_loss = F.cross_entropy(policy_logits, target_idx)
        value_loss = F.mse_loss(value_pred.squeeze(-1), value_targets[:, 0])
        
        total_loss = policy_loss + value_loss
        
        # Unroll
        for k in range(min(self.config.unroll_steps, seq_len - 1)):
            original_input = self.model.token_embed(obs)
            
            # Use action conditioning
            action_emb = self.model.token_embed(actions[:, k]).unsqueeze(1)
            h = hidden + action_emb.expand(-1, hidden.size(1), -1)
            
            for step in range(2):  # Quick thinking for dynamics
                h = self.model.think_block(h, original_input, step)
            
            h = self.model.ln_out(h)
            pooled = h.mean(dim=1)
            
            policy_logits = self.model.policy_head(pooled)
            value_pred = self.model.value_head(pooled)
            
            target_idx = policies[:, k+1].argmax(dim=-1)
            policy_loss = F.cross_entropy(policy_logits, target_idx)
            value_loss = F.mse_loss(value_pred.squeeze(-1), value_targets[:, k+1])
            
            total_loss = total_loss + policy_loss + value_loss
            hidden = h
        
        # Think penalty
        total_loss = total_loss + think_steps * self.config.think_penalty
        
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        self.optimizer.step()
        
        return total_loss.item()
    
    def train(self, max_samples: int = 500000, log_interval: int = 10000) -> Dict:
        print(f"\n{'='*70}")
        print(f"Starting ThinkSort v3 Training")
        print(f"Parameters: {self.model.count_parameters():,}")
        print(f"Max samples: {max_samples:,}")
        print(f"{'='*70}\n")
        
        start_time = time.time()
        best_acc = 0.0
        recent_losses = deque(maxlen=100)
        
        while self.total_samples < max_samples:
            accuracy, think_steps, confidence = self.collect_batch()
            
            if len(self.buffer['obs']) >= 2:
                for _ in range(4):
                    loss = self.train_step()
                    recent_losses.append(loss)
            
            # Curriculum
            avg_acc = np.mean(self.recent_accuracies) if self.recent_accuracies else 0
            if avg_acc >= self.accuracy_threshold and self.current_seq_length < self.max_seq_length:
                old_len = self.current_seq_length
                new_len = min(self.current_seq_length * 2, self.max_seq_length)
                
                print(f"\nüéØ Accuracy {avg_acc:.2%} >= {self.accuracy_threshold:.0%}")
                print(f"   ‚¨ÜÔ∏è CURRICULUM: {old_len} ‚Üí {new_len} numbers")
                
                self._recreate_env(new_len)
                self.recent_accuracies.clear()
            
            # Logging
            if self.total_samples % log_interval < self.config.num_parallel_envs * self.current_seq_length:
                avg_acc = np.mean(self.recent_accuracies) if self.recent_accuracies else 0
                avg_think = np.mean(self.recent_think_steps) if self.recent_think_steps else 0
                avg_conf = np.mean(self.recent_confidence) if self.recent_confidence else 0
                avg_loss = np.mean(recent_losses) if recent_losses else 0
                elapsed = time.time() - start_time
                speed = self.total_samples / elapsed
                
                print(f"Samples: {self.total_samples:8,} | "
                      f"N={self.current_seq_length:2} | "
                      f"Acc: {avg_acc:.2%} | "
                      f"Think: {avg_think:.1f} | "
                      f"Conf: {avg_conf:.3f} | "
                      f"Loss: {avg_loss:.3f} | "
                      f"{speed:,.0f}/s")
                
                if avg_acc > best_acc:
                    best_acc = avg_acc
                
                self.think_metrics.add(
                    self.total_samples, self.current_seq_length,
                    avg_think, avg_conf, avg_acc
                )
        
        elapsed = time.time() - start_time
        final_acc = np.mean(list(self.recent_accuracies)[-50:]) if self.recent_accuracies else 0
        
        print(f"\n{'='*70}")
        print(f"Training Complete!")
        print(f"Final seq_length: {self.current_seq_length}")
        print(f"Final accuracy: {final_acc:.2%}")
        print(f"Parameters: {self.model.count_parameters():,}")
        print(f"Time: {elapsed:.1f}s")
        print(f"{'='*70}")
        
        self.think_metrics.print_summary()
        
        return {
            'final_seq_length': self.current_seq_length,
            'final_accuracy': final_acc,
            'params': self.model.count_parameters(),
        }
    
    @torch.no_grad()
    def evaluate(self, seq_length: int, num_envs: int = 1000) -> Tuple[float, float, float]:
        """Evaluate and return (accuracy, avg_think_steps, avg_confidence)."""
        self.model.eval()
        
        env = VectorizedSortingEnv(num_envs, seq_length, self.config.vocab_size, self.device)
        obs = env.reset()
        
        total_correct = 0
        total_think = 0
        total_conf = 0
        
        for step in range(seq_length):
            policy_logits, _, _, think_steps, confidence = self.model(
                obs, out_pos=step, adaptive_exit=True
            )
            actions = policy_logits.argmax(dim=-1).clamp(0, self.config.vocab_size - 1)
            _, _, correct, _ = env.step(actions)
            total_correct += correct.sum().item()
            total_think += think_steps
            total_conf += confidence
        
        return (
            total_correct / (num_envs * seq_length),
            total_think / seq_length,
            total_conf / seq_length
        )


def run_thinksort_v3(max_samples: int = 500000):
    """Run ThinkSort v3 experiment."""
    print("\n" + "="*70)
    print("  üß† ThinkSort v3: Global Workspace Universal Transformer")
    print("  Residual Input Injection + Adaptive Exit")
    print("="*70)
    
    config = ThinkSortConfig(
        d_model=64,
        n_heads=4,
        d_ff=256,
        vocab_size=10,
        min_think_steps=2,
        max_think_steps=20,
        confidence_threshold=0.9,
        step_wise_reward=True,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )
    
    trainer = ThinkSortTrainer(config)
    results = trainer.train(max_samples, log_interval=10000)
    
    print("\n" + "="*70)
    print("Evaluation: Adaptive Thinking in Action")
    print("="*70)
    
    for n in [4, 8, 16, 32]:
        acc, think, conf = trainer.evaluate(n, num_envs=500)
        print(f"  Sort {n:2} numbers: {acc:.2%} ({think:.1f} steps, {conf:.3f} confidence)")
    
    print("="*70 + "\n")
    
    return results


if __name__ == "__main__":
    run_thinksort_v3()
